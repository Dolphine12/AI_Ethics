{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrK_GD3Ji0OX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 1: Biased Hiring Tool**\n",
        "\n",
        "Scenario:\n",
        "Amazon’s AI recruiting tool penalized female candidates based on the analysis of resumes. The tool was intended to help Amazon streamline its hiring process, but it ended up perpetuating gender biases.\n",
        "\n",
        "Task 1: Identify the Source of Bias\n",
        "\n",
        "In order to identify the source of bias in this case, we need to consider the following:\n",
        "\n",
        "Training Data:\n",
        "\n",
        "Amazon’s AI tool was trained on resumes submitted to the company over a 10-year period. As the majority of applicants were male, this led the AI to develop a bias toward male-oriented language, qualifications, and experiences. For example, the model learned to favor resumes that used words such as \"executed\" or \"captain,\" which were more commonly used by male candidates.\n",
        "\n",
        "Model Design:\n",
        "\n",
        "The model was built to recognize patterns from past successful hiring decisions. However, by relying on historical data that reflected gender disparities, it inadvertently replicated these inequalities in its predictions.\n",
        "\n",
        "Source of Bias:\n",
        "\n",
        "The primary source of bias in Amazon's AI recruiting tool comes from biased training data. The historical dataset used to train the model contained inherent gender biases (i.e., more male candidates, male-dominated language), which led to discriminatory outcomes.\n",
        "\n",
        "Task 2:\n",
        "\n",
        "Propose Three Fixes to Make the Tool Fairer\n",
        "In order to mitigate this bias and make the AI recruitment tool more equitable, the following fixes can be proposed:\n",
        "\n",
        "Diversified Training Data:\n",
        "\n",
        "Fix: Ensure that the training dataset is representative of a diverse pool of candidates. This includes balancing gender, race, age, and other demographic factors to eliminate the systemic bias present in the model’s learning process.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "Curate a dataset that includes a more even gender representation, or consider data augmentation techniques to generate synthetic data from underrepresented groups.\n",
        "\n",
        "Bias Detection & Regular Audits:\n",
        "\n",
        "Fix: Implement a continuous monitoring system that audits the AI model for biases on an ongoing basis. This includes tracking and analyzing the outputs of the AI model across different demographic groups (e.g., gender, race).\n",
        "\n",
        "Implementation:\n",
        "\n",
        "Periodically test the tool on diverse test datasets to identify any biases in the outcomes. Create a feedback loop to retrain the model when necessary.\n",
        "\n",
        "Bias-Aware Algorithms:\n",
        "\n",
        "Fix: Modify the model to detect and correct for potential gender biases through bias-correction techniques, such as re-weighting the features, adversarial debiasing, or fairness constraints during training.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "Use fairness metrics like Demographic Parity or Equalized Odds to guide the model’s decision-making process. This ensures that the model does not unfairly penalize any particular group of candidates.\n",
        "\n",
        "Task 3:\n",
        "\n",
        " Suggest Metrics to Evaluate Fairness Post-Correction\n",
        "After implementing the fixes, it is essential to evaluate the fairness of the system. Here are three key metrics that could be used to assess the fairness of the modified tool:\n",
        "\n",
        "Demographic Parity:\n",
        "\n",
        "Definition: Demographic parity refers to the requirement that the acceptance rate of candidates from different demographic groups (e.g., male and female) should be the same.\n",
        "\n",
        "How to Measure: Calculate the acceptance rate for male and female candidates. Ensure the tool does not favor one group over another in terms of hiring decisions.\n",
        "\n",
        "Equalized Odds:\n",
        "\n",
        "Definition:\n",
        "\n",
        "Equalized odds ensures that the true positive rate (the proportion of candidates who are correctly identified as suitable) and false positive rate (the proportion of candidates incorrectly rejected) are equal across demographic groups.\n",
        "\n",
        "How to Measure:\n",
        "\n",
        "For each group (e.g., male and female), calculate the true positive rate (TPR) and false positive rate (FPR). These rates should be similar for all groups to ensure fairness.\n",
        "\n",
        "Disparate Impact:\n",
        "\n",
        "Definition: Disparate impact refers to a situation where a decision-making process disproportionately affects one group more than others.\n",
        "\n",
        "How to Measure:\n",
        "\n",
        "The 80% rule can be applied. This rule states that if a group’s acceptance rate is less than 80% of the acceptance rate of the most favored group, it’s a sign of disparate impact.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "\n",
        "The case of Amazon’s biased hiring tool demonstrates the importance of designing AI systems that are fair, transparent, and inclusive. By addressing bias in training data, implementing ongoing audits, and utilizing fairness metrics, we can work towards AI systems that are both responsible and equitable.\n",
        "\n",
        "It is crucial to integrate ethical considerations into AI development to prevent harmful biases from taking root, ensuring that AI technologies are not only effective but also socially responsible.\n",
        "\n",
        "References:\n",
        "Dastin, J. (2018). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters.\n",
        "\n",
        "Binns, R. (2018). Fairness in machine learning: Lessons from Amazon’s AI hiring tool. Medium."
      ],
      "metadata": {
        "id": "c4wvzQQCjX8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JYTsN-6tjdar"
      }
    }
  ]
}